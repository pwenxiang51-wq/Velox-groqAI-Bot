/**
 * âš¡ Velo.x AI - v5.0 (Final Ultimate Edition)
 * ---------------------------------------------
 * @module 1: Groq Llama 3.3 (å¯¹è¯ + è®°å¿†)
 * @module 2: Stable Diffusion XL (å†™å®ç”»å›¾)
 * @module 3: TG Bot API (äº¤äº’æ ¸å¿ƒ)
 */

const CONFIG = {
    // æ–‡æœ¬å¤§è„‘
    TEXT_MODEL: 'llama-3.3-70b-versatile',
    SYSTEM_PROMPT: `You are Velo.x AI, a helpful assistant. Answer concisely in Chinese. Use Markdown.`,
    // è®°å¿†é™åˆ¶ (æ»‘åŠ¨çª—å£)
    MEMORY_LIMIT: 12,
    // ç»˜å›¾å¼•æ“
    IMAGE_MODEL: '@cf/stabilityai/stable-diffusion-xl-base-1.0'
};

export default {
    async fetch(request, env) {
        // Webhook æ¡æ‰‹
        if (request.method !== 'POST') return new Response('Velo.x AI System Online.', { status: 200 });

        try {
            const update = await request.json();
            if (!update.message || !update.message.text) return new Response('OK');

            const chatId = update.message.chat.id;
            const userId = update.message.from.id;
            const text = update.message.text.trim();
            const messageId = update.message.message_id;

            // ğŸ›¡ï¸ 1. å®‰å…¨é‰´æƒ (åªæœåŠ¡å¤§ä½¬)
            if (env.ADMIN_ID && String(userId) !== String(env.ADMIN_ID)) {
                return new Response('OK'); 
            }

            // ğŸ§¹ 2. æ¸…é™¤è®°å¿†æŒ‡ä»¤
            if (text === '/clear' || text === '/reset') {
                try {
                    await env.MEMORY.delete(String(chatId));
                    await sendText(chatId, "ğŸ§¹ **å¤§è„‘å·²æ ¼å¼åŒ–ï¼Œè®°å¿†å·²æ¸…é™¤ã€‚**", env);
                } catch (e) {
                    await sendText(chatId, "âš ï¸ æ¸…é™¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ˜¯å¦ç»‘å®šäº† KV (å˜é‡å MEMORY)", env);
                }
                return new Response('OK');
            }

            // ğŸ¨ 3. ç»˜å›¾æŒ‡ä»¤ (/img)
            if (text.startsWith('/img') || text.startsWith('/draw')) {
                const prompt = text.replace(/^\/(img|draw)\s*/, '');
                
                if (!prompt) {
                    await sendText(chatId, "âš ï¸ è¯·è¾“å…¥æç¤ºè¯ï¼Œä¾‹å¦‚ï¼š`/img èµ›åšæœ‹å…‹`", env);
                    return new Response('OK');
                }

                // å‘é€â€œä¸Šä¼ ä¸­â€çŠ¶æ€
                await sendChatAction(chatId, 'upload_photo', env);
                // è°ƒç”¨å†™å®ç”»å›¾é€»è¾‘
                await handleImageGeneration(chatId, prompt, messageId, env);
            } 
            
            // ğŸ’¬ 4. å¯¹è¯æŒ‡ä»¤ (Groq + è®°å¿†)
            else {
                await sendChatAction(chatId, 'typing', env);
                
                let history = [];
                try {
                    // å°è¯•è¯»å–è®°å¿†
                    history = await env.MEMORY.get(String(chatId), { type: 'json' }) || [];
                } catch (e) {
                    // å¦‚æœæ²¡ç»‘ KVï¼Œå°±å½“ä½œæ²¡æœ‰è®°å¿†ç»§ç»­è·‘ï¼Œä¸æŠ¥é”™
                    history = [];
                }
                
                // æ‹¼æ¥å½“å‰é—®é¢˜
                const requestMessages = [...history, { role: "user", content: text }];
                
                // è°ƒç”¨ Groq
                const aiReply = await fetchGroqWithHistory(requestMessages, env);
                
                // å‘é€å›å¤
                await sendText(chatId, aiReply, env, messageId);

                // æ›´æ–°å¹¶ä¿å­˜è®°å¿†
                try {
                    history.push({ role: "user", content: text });
                    history.push({ role: "assistant", content: aiReply });
                    // è£å‰ªè¶…é•¿è®°å¿†
                    if (history.length > CONFIG.MEMORY_LIMIT) {
                        history = history.slice(history.length - CONFIG.MEMORY_LIMIT);
                    }
                    await env.MEMORY.put(String(chatId), JSON.stringify(history));
                } catch (e) {
                    // KV ä¿å­˜å¤±è´¥å¿½ç•¥ï¼Œä¸å½±å“å¯¹è¯
                }
            }

        } catch (e) {
            console.error(e);
        }
        return new Response('OK');
    }
};

// ==========================================
// ğŸ§  æ ¸å¿ƒåŠŸèƒ½ï¼šGroq å¯¹è¯ (Llama 3.3)
// ==========================================
async function fetchGroqWithHistory(messages, env) {
    try {
        const payloadMessages = [
            { role: "system", content: CONFIG.SYSTEM_PROMPT },
            ...messages
        ];

        const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${env.GROQ_API_KEY}`
            },
            body: JSON.stringify({
                model: CONFIG.TEXT_MODEL,
                messages: payloadMessages,
                temperature: 0.6, // 0.6 æ¯”è¾ƒå‡è¡¡ï¼Œé€‚åˆå¯¹è¯
                max_tokens: 2048
            })
        });
        const data = await response.json();
        return data.choices?.[0]?.message?.content || "âŒ Groq æ— å“åº”";
    } catch (e) {
        return `Groq Error: ${e.message}`;
    }
}

// ==========================================
// ğŸ¨ æ ¸å¿ƒåŠŸèƒ½ï¼šAI ç»˜å›¾ (å†™å®å¢å¼ºç‰ˆ V2)
// ==========================================
async function handleImageGeneration(chatId, prompt, replyId, env) {
    try {
        // âœ¨ è‡ªåŠ¨æ³¨å…¥â€œç”»è´¨å¢å¼ºå‰‚â€
        const enhancedPrompt = prompt + ", photorealistic, 8k resolution, cinematic lighting, highly detailed, masterpiece, sharp focus";
        
        // è°ƒç”¨ CF æ˜¾å¡
        const inputs = { prompt: enhancedPrompt, steps: 25 }; // 25æ­¥æ›´ç»†è…»
        const responseStream = await env.AI.run(CONFIG.IMAGE_MODEL, inputs);

        // æ ¼å¼è½¬æ¢ (ArrayBuffer -> Blob)
        const arrayBuffer = await new Response(responseStream).arrayBuffer();
        const blob = new Blob([arrayBuffer], { type: 'image/png' });

        // æ‰“åŒ…å‘é€ç»™ TG
        const formData = new FormData();
        formData.append('chat_id', chatId);
        formData.append('photo', blob, 'gen.png'); 
        formData.append('caption', `ğŸ¨ \`${prompt}\``);
        formData.append('parse_mode', 'Markdown');
        formData.append('reply_to_message_id', replyId);

        const res = await fetch(`https://api.telegram.org/bot${env.TG_BOT_TOKEN}/sendPhoto`, {
            method: 'POST',
            body: formData
        });

        if (!res.ok) throw new Error(await res.text());

    } catch (err) {
        await sendText(chatId, `âŒ **ç»˜å›¾å¤±è´¥:** ${err.message}`, env);
    }
}

// ==========================================
// ğŸ› ï¸ è¾…åŠ©å·¥å…·
// ==========================================
async function sendChatAction(chatId, action, env) {
    await fetch(`https://api.telegram.org/bot${env.TG_BOT_TOKEN}/sendChatAction`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ chat_id: chatId, action: action })
    });
}

async function sendText(chatId, text, env, replyId = null) {
    const url = `https://api.telegram.org/bot${env.TG_BOT_TOKEN}/sendMessage`;
    const payload = { chat_id: chatId, text: text, parse_mode: 'Markdown', reply_to_message_id: replyId };
    let res = await fetch(url, { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify(payload) });
    // é™çº§é‡è¯• (é˜²æ­¢ Markdown æ ¼å¼é”™è¯¯)
    if (!res.ok) {
        delete payload.parse_mode;
        await fetch(url, { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify(payload) });
    }
}
